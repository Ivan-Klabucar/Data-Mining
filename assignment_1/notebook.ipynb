{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is about a method to compare bodies of texts and measure their similarities. For this we use shingling and Jaccard similarity.\n",
    "To test the implementation and conduct some experiments we use a dataset containing a number of documents and some mixtures which we\n",
    "added ourselves.\n",
    "\n",
    "Document 'MIX_OHIO_BROWN_CHEMLAWN.txt' is a mix of documents 'OHIO_MATTRESS.txt', 'BROWN_FORMAN.txt', and 'CHEMLAWN.txt'\n",
    "Document 'MIX_CHEMLAWN_SHAMROCK.txt' is 'DIAMOND_SHAMROCK.txt' inserted into 'CHEMLAWN.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the documents and save them in a dictionary, using their respective file names as key (while removing the .txt ending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "print('Documents in dataset:\\n')\n",
    "for i, filename in enumerate(os.listdir(\"dataset\")):\n",
    "    print(f'{i} {filename}')\n",
    "    with open(os.path.join(\"dataset\", filename), 'r') as f:\n",
    "        data[filename[:-4]] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we shingle all documents using a shingle length k of 9. In the shingle function we prepare each document by removing all punctuations and transforming everything to lower case text.\n",
    "We then take substrings of length k and hash them using the adler32 hash function from the library zlib which returns a 4-byte hash of any string.\n",
    "The hashed shingles of each document are saved in a set to remove duplicate shingles and then stored in another dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up comparisons we use a technique called min-hashing where we create signatures for documents of variable length n.\n",
    "For this we create n different random hash functions of the form (ax + b) % c with random a and b and a fixed c. Using these signatures only allows us to calculate an estimate of the jaccard similarity however. Hence we're trading performance with accuracy\n",
    "\n",
    "We apply this min-hashing technique with n=200 and store the signatures in another dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shingling the dataset\n",
    "list_of_data = [c for c in data]\n",
    "shingled_data = dict()\n",
    "THRES = 0.07\n",
    "for c in data:\n",
    "    shingled_data[c] = shingle(data[c], k=9)\n",
    "    \n",
    "#Creating signatures for the dataset\n",
    "min_hasher = MinHash(n=200)\n",
    "signed_data = dict()\n",
    "for c in data:\n",
    "    signed_data[c] = min_hasher.create_signature(shingled_data[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the \"heavy worker methods\" in an extra python file but create here some shortcuts which abstract the methods to make it more user-friendly for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for searching for similar pairs\n",
    "def jaccard(c1, c2):\n",
    "    return compareSets(shingled_data[c1], shingled_data[c2])\n",
    "\n",
    "def jaccard_estimate(c1, c2, signatures=signed_data):\n",
    "    return compare_signatures(signatures[c1], signatures[c2])\n",
    "\n",
    "# here we create a table with all documents as rows and columns and their \n",
    "# respective value of similarity measured with the given method\n",
    "def calculate_similarity_of_all_pairs(method, documents=list_of_data):\n",
    "    result = pd.DataFrame(columns=documents)\n",
    "    for row in documents:\n",
    "        result[row] = [method(row, col) for col in documents]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns for a given similarity table all pairs which are above a given threshold\n",
    "def find_pairs_above_threshold(df, threshold, documents=list_of_data):\n",
    "    checked = set()\n",
    "    result = set()\n",
    "    for col in df:\n",
    "        checked.add(col)\n",
    "        for i, sim in enumerate(df[col]):\n",
    "            curr = documents[i]\n",
    "            # similarity is symmetrical, hence we only check about half of the table\n",
    "            if curr in checked: continue\n",
    "            if sim > threshold: result.add((col, curr))\n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Study in true/false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we add some functions to visualize our results. \n",
    "- First a method to plot the similarity in a confusion-matrix-like fashion. The brighter a cell is, the more similar two documents are\n",
    "- And second a simple formatted print statement to output the similar pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(mat):\n",
    "    plt.figure(dpi=200)\n",
    "    plt.matshow(mat)\n",
    "    plt.yticks(range(len(list_of_data)), list_of_data, rotation=\"horizontal\")\n",
    "    plt.xticks(range(len(list_of_data)), list_of_data, rotation=\"vertical\")\n",
    "    plt.show()\n",
    "\n",
    "def print_pairs(pairs):\n",
    "    print(\"\\n\".join([f\"{r[0]} - {r[1]}\" for r in pairs]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculated the exact jaccard similarity, plotted the results with the previous function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the Jaccard similarity for every pair of documents\n",
    "jaccard_sim_map = calculate_similarity_of_all_pairs(jaccard)\n",
    "\n",
    "plot_matrix(jaccard_sim_map)\n",
    "\n",
    "gt_result = find_pairs_above_threshold(jaccard_sim_map, THRES)\n",
    "print_pairs(gt_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and did the same with the estimated jaccard similarity ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimating the Jaccard similarity for every pair of documents\n",
    "jaccard_sim_estim_map = calculate_similarity_of_all_pairs(jaccard_estimate)\n",
    "plot_matrix(jaccard_sim_estim_map)\n",
    "\n",
    "est_result = find_pairs_above_threshold(jaccard_sim_estim_map, THRES)\n",
    "print_pairs(est_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and looked at true/false positives/negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results of estimate')\n",
    "# counting the pairs which are in both sets\n",
    "acum = 0\n",
    "for p in est_result:\n",
    "    if p in gt_result or (p[1], p[0]) in gt_result: \n",
    "        acum += 1\n",
    "print(f'Length of estimated result: {len(est_result)}')\n",
    "print(f'Length of real result: {len(gt_result)}')\n",
    "print(f'Found {acum}/{len(gt_result)}')  # From these two lines we get information about true/false positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are a lot of documents and we want to calculate the pairwise similarities, the number of calculations grows quadratic with the number of documents. Therefore, we preselect candidates to calculate the estimated jaccard similarity to reduce the number of needed calculations. However, we again have to trade performance with accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we use LSH, where the signature of a document is divided into bands which are then hashed. Finally we calculate the (estimated) jaccard similarity only for those pairs of documents for which at least one of their respective bands got hashed into the same bucket. \n",
    "The implementation for this can again be found in functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we conduct the same experiment as before but of course don't plot a matrix since the whole point is not to compare every document with every other document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally sensitive hashing\n",
    "lsh = LSH(signatures=signed_data, num_of_bands=50, threshold=THRES)\n",
    "lsh_result = lsh.get_candidates_above_threshold(jaccard_estimate)\n",
    "\n",
    "\n",
    "print('Results of LSH')                                                         # b = num_of_bands\n",
    "for p in lsh_result:                                                            # t = threshold, r = rows within a band\n",
    "    print(f'{jaccard_sim_estim_map.loc[list_of_data.index(p[0]), p[1]]} {p}')   # explain results with 1 - (1 - t^r)^b\n",
    "print()\n",
    "    \n",
    "acum = 0\n",
    "for p in lsh_result:\n",
    "    if p in gt_result or (p[1], p[0]) in gt_result: acum += 1\n",
    "print(f'Length of estimated result: {len(lsh_result)}')\n",
    "print(f'Length of real result: {len(gt_result)}')\n",
    "print(f'Found {acum}/{len(gt_result)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of part 1 we implemented and explored the full pipeline described in the lecture: shingling -> minhashing -> lsh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Study in scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to get a sense of the speed of execution by applying the above described algorithms to a dataset of increasing size. We will construct the larger data sets by repeatedly copying the original back into the larger one. The larger data set will contain duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we defined an easy-to-use function to increase the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_extend(d_to_extend, with_d, time):\n",
    "    for k in with_d:\n",
    "        d_to_extend[k + f'-{time}'] = with_d[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then steadily increase the dataset size and look for similar documents for the respective dataset and take the time for:\n",
    "1. the jaccard-estimate using only the signatures\n",
    "2. the jaccard-estimate but preselected using lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_time = []\n",
    "lsh_time = []\n",
    "\n",
    "NUM_OF_EXPERIMENTS = 30\n",
    "larger_data = dict()\n",
    "for i in range(NUM_OF_EXPERIMENTS):\n",
    "    # increase of dataset size\n",
    "    dict_extend(d_to_extend=larger_data, with_d=signed_data, time=i)\n",
    "    doc_list = [k for k in larger_data]\n",
    "    \n",
    "    # finding similar pairs using the jaccard_estimate\n",
    "    start = time.time()\n",
    "    res = calculate_similarity_of_all_pairs(                                  \\\n",
    "                lambda a,b: jaccard_estimate(a, b, signatures=larger_data),   \\\n",
    "                documents=doc_list                                            \\\n",
    "    )\n",
    "    final = find_pairs_above_threshold(res, THRES, documents=doc_list)\n",
    "    end = time.time()\n",
    "    signature_time.append(end - start)\n",
    "    \n",
    "    # finding similar pairs but with lsh pre-selection\n",
    "    start = time.time()\n",
    "    lsh = LSH(signatures=larger_data, num_of_bands=50, threshold=THRES)\n",
    "    lsh_result = lsh.get_candidates_above_threshold(jaccard_estimate, data_with_duplicates=True)\n",
    "    end = time.time()\n",
    "    lsh_time.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the run-times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i*15 for i in range(1,NUM_OF_EXPERIMENTS+1)]\n",
    "plt.figure(1)\n",
    "plt.plot(x, signature_time, label='Signatures')\n",
    "plt.plot(x, lsh_time, label='LSH')\n",
    "plt.ylabel('Execution time in s')\n",
    "plt.xlabel('Size of data (N)')\n",
    "plt.legend()\n",
    "plt.title('Scalability comparison of LSH and pair-wise similarity estimation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see the speed-up with increasing dataset size, when using lsh. We also can see the quadratic increase in run-time when we do not use the lsh preselection while lsh's run-time appears to increase linearly."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
